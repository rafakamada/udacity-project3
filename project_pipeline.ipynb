{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from project3_iam_user import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_CLUSTER_TYPE = 'single-node'\n",
    "DWH_NUM_NODES = 2\n",
    "DWH_NODE_TYPE = 'dc2.large'\n",
    "DWH_CLUSTER_IDENTIFIER = 'project3-cluster'\n",
    "DWH_DB = 'project3_db'\n",
    "DWH_DB_USER = 'p3user'\n",
    "DWH_DB_PASSWORD = 'p3Passw0rd'\n",
    "DWH_PORT = 5439\n",
    "DWH_IAM_ROLE_NAME =  'project3_redshift_role'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create clients for IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',aws_access_key_id=ACCESS_KEY_ID,\n",
    "                     aws_secret_access_key=SECRET_ACCESS_KEY,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=ACCESS_KEY_ID,\n",
    "                       aws_secret_access_key=SECRET_ACCESS_KEY\n",
    "                       )\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=ACCESS_KEY_ID,\n",
    "                       aws_secret_access_key=SECRET_ACCESS_KEY\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IAM Role for Redshift\n",
    "Make Redshift able to read s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name project3_redshift_role already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::929805762734:role/project3_redshift_role\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "#Create the role, \n",
    "try: #it might exists already\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "dwh_role_arn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(dwh_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Redshift cluster\n",
    "Create a Redshift cluster using the preset parameters and associate the created role to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType = DWH_CLUSTER_TYPE,\n",
    "        NodeType = DWH_NODE_TYPE,\n",
    "        #NumberOfNodes = DWH_NUM_NODES,\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName = DWH_DB,\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername = DWH_DB_USER,\n",
    "        MasterUserPassword = DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[dwh_role_arn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cluster properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the cluster is ready\n",
    "waiter = redshift.get_waiter('cluster_available')\n",
    "waiter.wait( \n",
    "    ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "    WaiterConfig={\n",
    "        'Delay': 30,\n",
    "        'MaxAttempts': 20\n",
    "    }\n",
    ")\n",
    "\n",
    "#get properties dictionary\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a TCP port to access the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-0006eb2f148068d87')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write cluster props to cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = f\"\"\"[CLUSTER]\n",
    "HOST='{myClusterProps['Endpoint']['Address']}'\n",
    "DB_NAME='{DWH_DB}'\n",
    "DB_USER='{DWH_DB_USER}'\n",
    "DB_PASSWORD='{DWH_DB_PASSWORD}'\n",
    "DB_PORT='{myClusterProps['Endpoint']['Port']}'\n",
    "\n",
    "[IAM_ROLE]\n",
    "ARN='{dwh_role_arn}'\n",
    "\n",
    "[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'\n",
    "\"\"\"\n",
    "\n",
    "f = open('dwh.cfg', 'w')\n",
    "f.write(file_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starrt\n",
      "<Section: CLUSTER>\n",
      "conexao criada\n",
      "cursor criado\n",
      "DROP TABLE IF EXISTS staging_events\n",
      "DROP TABLE IF EXISTS staging_songs\n",
      "DROP TABLE IF EXISTS songplays\n",
      "DROP TABLE IF EXISTS users CASCADE\n",
      "DROP TABLE IF EXISTS songs CASCADE\n",
      "DROP TABLE IF EXISTS artists CASCADE\n",
      "DROP TABLE IF EXISTS time CASCADE\n",
      "tabelas dropadas\n",
      "\n",
      "CREATE TABLE staging_events (\n",
      "  artist VARCHAR,\n",
      "  auth VARCHAR,\n",
      "  firstName VARCHAR,\n",
      "  gender VARCHAR,\n",
      "  itemInSession INT,\n",
      "  lastName VARCHAR,\n",
      "  length FLOAT,\n",
      "  level VARCHAR,\n",
      "  location VARCHAR,\n",
      "  method VARCHAR,\n",
      "  page VARCHAR,\n",
      "  registration FLOAT,\n",
      "  sessionID INT,\n",
      "  song VARCHAR,\n",
      "  status INT,\n",
      "  ts INT,\n",
      "  userAgent VARCHAR,\n",
      "  userID INT\n",
      ")\n",
      "\n",
      "\n",
      "CREATE TABLE staging_songs (\n",
      "  num_songs INT,\n",
      "  artist_id VARCHAR,\n",
      "  artist_latitude FLOAT,\n",
      "  artist_longitude FLOAT,\n",
      "  artist_location VARCHAR,\n",
      "  artist_name VARCHAR,\n",
      "  song_id VARCHAR,\n",
      "  title VARCHAR,\n",
      "  duration FLOAT,\n",
      "  year INT\n",
      ")\n",
      "\n",
      "\n",
      "CREATE TABLE users (\n",
      "  id  INT PRIMARY KEY,\n",
      "  first_name VARCHAR,\n",
      "  last_name VARCHAR,\n",
      "  gender VARCHAR,\n",
      "  level VARCHAR\n",
      ")\n",
      "\n",
      "\n",
      "CREATE TABLE songs (\n",
      "  id VARCHAR PRIMARY KEY,\n",
      "  title VARCHAR,\n",
      "  artist_id VARCHAR,\n",
      "  year INT,\n",
      "  duration FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "CREATE TABLE artists (\n",
      "  id VARCHAR PRIMARY KEY,\n",
      "  name VARCHAR,\n",
      "  location VARCHAR,\n",
      "  latitude FLOAT,\n",
      "  longitude FLOAT\n",
      ")\n",
      "\n",
      "\n",
      "CREATE TABLE time (\n",
      "  start_time TIMESTAMP PRIMARY KEY,\n",
      "  hour INT,\n",
      "  day INT,\n",
      "  week INT,\n",
      "  month INT,\n",
      "  year INT,\n",
      "  weekday INT\n",
      ")\n",
      "\n",
      " \n",
      "CREATE TABLE songplays (\n",
      "  id INT IDENTITY(0,1),\n",
      "  start_time TIMESTAMP,\n",
      "  user_id INT,\n",
      "  level VARCHAR,\n",
      "  song_id VARCHAR,\n",
      "  artist_id VARCHAR,\n",
      "  session_id INT,\n",
      "  location VARCHAR,\n",
      "  user_agent VARCHAR,\n",
      "  FOREIGN KEY (start_time) REFERENCES time(start_time),\n",
      "  FOREIGN KEY (user_id) REFERENCES users(id),\n",
      "  FOREIGN KEY (song_id) REFERENCES songs(id),\n",
      "  FOREIGN KEY (artist_id) REFERENCES artists(id)\n",
      ")\n",
      "\n",
      "tabelas criadas\n"
     ]
    }
   ],
   "source": [
    "! python3 create_tables.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'project3-cluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'p3user',\n",
       "  'DBName': 'project3_db',\n",
       "  'Endpoint': {'Address': 'project3-cluster.cqh4kd6a34p7.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2021, 12, 6, 2, 35, 15, 942000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0006eb2f148068d87',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-0ab9503a980b66838',\n",
       "  'AvailabilityZone': 'us-west-2c',\n",
       "  'PreferredMaintenanceWindow': 'sun:07:30-sun:08:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 1,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::929805762734:role/project3_redshift_role',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2021, 12, 12, 7, 30, tzinfo=tzutc())},\n",
       " 'ResponseMetadata': {'RequestId': '0e3a8aaf-a5d7-459e-830c-c374cb8198c0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0e3a8aaf-a5d7-459e-830c-c374cb8198c0',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2656',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Mon, 06 Dec 2021 02:35:36 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier='project3-cluster',  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
