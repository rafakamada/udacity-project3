{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from project3_iam_user import * #import ACCESS_KEY_ID and SECRET_ACCESS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_CLUSTER_TYPE = 'single-node'\n",
    "#DWH_NUM_NODES = 2\n",
    "DWH_NODE_TYPE = 'dc2.large'\n",
    "DWH_CLUSTER_IDENTIFIER = 'project3-cluster'\n",
    "DWH_DB = 'project3_db'\n",
    "DWH_DB_USER = 'p3user'\n",
    "DWH_DB_PASSWORD = 'p3Passw0rd'\n",
    "DWH_PORT = 5439\n",
    "DWH_IAM_ROLE_NAME =  'project3_redshift_role'\n",
    "REGION = 'us-west-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create clients for IAM and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',aws_access_key_id=ACCESS_KEY_ID,\n",
    "                     aws_secret_access_key=SECRET_ACCESS_KEY,\n",
    "                     region_name=REGION\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=ACCESS_KEY_ID,\n",
    "                       aws_secret_access_key=SECRET_ACCESS_KEY\n",
    "                       )\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=ACCESS_KEY_ID,\n",
    "                       aws_secret_access_key=SECRET_ACCESS_KEY\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IAM Role for Redshift\n",
    "Make Redshift able to read s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name project3_redshift_role already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::929805762734:role/project3_redshift_role\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "#Create the role, \n",
    "try: #it might exists already\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "dwh_role_arn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(dwh_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Redshift cluster\n",
    "Create a Redshift cluster using the preset parameters and associate the created role to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType = DWH_CLUSTER_TYPE,\n",
    "        NodeType = DWH_NODE_TYPE,\n",
    "        #NumberOfNodes = DWH_NUM_NODES,\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName = DWH_DB,\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername = DWH_DB_USER,\n",
    "        MasterUserPassword = DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[dwh_role_arn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cluster properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the cluster is ready\n",
    "waiter = redshift.get_waiter('cluster_available')\n",
    "waiter.wait( \n",
    "    ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "    WaiterConfig={\n",
    "        'Delay': 30,\n",
    "        'MaxAttempts': 20\n",
    "    }\n",
    ")\n",
    "\n",
    "#get properties dictionary\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open a TCP port to access the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-0006eb2f148068d87')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try: #the rule might already exist\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write cluster props to cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = f\"\"\"[CLUSTER]\n",
    "HOST='{myClusterProps['Endpoint']['Address']}'\n",
    "DB_NAME='{DWH_DB}'\n",
    "DB_USER='{DWH_DB_USER}'\n",
    "DB_PASSWORD='{DWH_DB_PASSWORD}'\n",
    "DB_PORT='{myClusterProps['Endpoint']['Port']}'\n",
    "\n",
    "[IAM_ROLE]\n",
    "ARN={dwh_role_arn}\n",
    "\n",
    "[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'\n",
    "\n",
    "[REGION]\n",
    "REGION='{REGION}'\n",
    "\"\"\"\n",
    "\n",
    "f = open('dwh.cfg', 'w')\n",
    "f.write(file_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 create_tables.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting load\n",
      "starting insert\n",
      "\n",
      "INSERT INTO users (id, first_name, last_name, gender, level)\n",
      "SELECT userid,\n",
      "       firstname,\n",
      "       lastname,\n",
      "       gender,\n",
      "       level\n",
      "FROM staging_events\n",
      "WHERE page = 'NextSong'\n",
      "\n",
      "\n",
      "INSERT INTO songs (id, title, artist_id, year, duration)\n",
      "SELECT song_id,\n",
      "       title,\n",
      "       artist_id,\n",
      "       year,\n",
      "       duration\n",
      "FROM staging_songs\n",
      "\n",
      "\n",
      "INSERT INTO artists (id, name, location, latitude, longitude)\n",
      "SELECT artist_id,\n",
      "       artist_name,\n",
      "       artist_location,\n",
      "       artist_latitude,\n",
      "       artist_longitude\n",
      "FROM staging_songs\n",
      "\n",
      "\n",
      "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
      "SELECT TIMESTAMP 'epoch' + ts::BIGINT/1000 * INTERVAL '1 second' AS epoch_to_timestamp,\n",
      "       DATE_PART('hour', epoch_to_timestamp),\n",
      "       DATE_PART('day', epoch_to_timestamp),\n",
      "       DATE_PART('week', epoch_to_timestamp),\n",
      "       DATE_PART('month', epoch_to_timestamp),\n",
      "       DATE_PART('year', epoch_to_timestamp),\n",
      "       DATE_PART('weekday', epoch_to_timestamp)\n",
      "FROM staging_events\n",
      "WHERE page = 'NextSong'\n",
      "\n",
      "\n",
      "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
      "SELECT TIMESTAMP 'epoch' + e.ts::BIGINT/1000 * INTERVAL '1 second',\n",
      "       e.userid,\n",
      "       e.level,\n",
      "       s.id,\n",
      "       s.artist_id,\n",
      "       e.sessionid,\n",
      "       e.location,\n",
      "       e.useragent\n",
      "FROM staging_events AS e\n",
      "  INNER JOIN songs AS s ON e.song = s.title\n",
      "                       AND e.page = 'NextSong'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python3 etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'project3-cluster',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'p3user',\n",
       "  'DBName': 'project3_db',\n",
       "  'Endpoint': {'Address': 'project3-cluster.cqh4kd6a34p7.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2021, 12, 27, 21, 10, 10, 326000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0006eb2f148068d87',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-0ab9503a980b66838',\n",
       "  'AvailabilityZone': 'us-west-2c',\n",
       "  'PreferredMaintenanceWindow': 'thu:08:30-thu:09:00',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 1,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::929805762734:role/project3_redshift_role',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2021, 12, 30, 8, 30, tzinfo=tzutc())},\n",
       " 'ResponseMetadata': {'RequestId': '42bc81cf-ad8b-4859-98b3-653923d583e1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '42bc81cf-ad8b-4859-98b3-653923d583e1',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2736',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Mon, 27 Dec 2021 21:38:07 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier='project3-cluster',  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
